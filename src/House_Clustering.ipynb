{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Get an active session from snowflake.snowpark\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from snowflake.snowpark.functions import col, hour\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "TIME_COL = \"TIME_COL\"   \n",
    "PARENT_COL = \"PARENT_NAME\"  \n",
    "TABLE_NAME = \"vitalise_data_light_01\"\n",
    "MIN_VARIANCE_EXPLAINED = 0.90  \n",
    "MIN_OCCURRENCE_THRESHOLD = 0.001\n",
    "\n",
    "class PCAAnalyzer:\n",
    "    \"\"\"PCA analysis for sparse recording hours\"\"\"\n",
    "    \n",
    "    def __init__(self, min_variance=0.90, min_occurrence=0.001):\n",
    "        self.min_variance = min_variance\n",
    "        self.min_occurrence = min_occurrence\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = None\n",
    "        self.variance_selector = None\n",
    "        self.actual_hours = None  # Will store actual available hours\n",
    "        \n",
    "    def discover_actual_recording_hours(self, session):\n",
    "        \"\"\"Discover which hours actually have data\"\"\"\n",
    "        print(\"DISCOVERING ACTUAL RECORDING HOURS\")\n",
    "        print(\"*\" * 50)\n",
    "        \n",
    "        # Check which hours have data\n",
    "        hour_check = session.sql(f\"\"\"\n",
    "            SELECT HOUR({TIME_COL}) as hour, COUNT(*) as records \n",
    "            FROM {TABLE_NAME} \n",
    "            WHERE {PARENT_COL} IS NOT NULL\n",
    "            GROUP BY HOUR({TIME_COL}) \n",
    "            ORDER BY hour\n",
    "        \"\"\").to_pandas()\n",
    "        \n",
    "        self.actual_hours = sorted(hour_check['HOUR'].tolist())\n",
    "        \n",
    "        print(f\"Actual Recording Hours Found:\")\n",
    "        for _, row in hour_check.iterrows():\n",
    "            print(f\"   • Hour {row['HOUR']:2d}: {row['RECORDS']:,} records\")\n",
    "            \n",
    "        print(f\"Analysis Summary:\")\n",
    "        print(f\"Total hours with data: {len(self.actual_hours)}\")\n",
    "        print(f\"Hour range: {min(self.actual_hours)}-{max(self.actual_hours)}\")\n",
    "        print(f\"Missing hours: {set(range(24)) - set(self.actual_hours)}\")\n",
    "        \n",
    "        # Identify gaps\n",
    "        missing_hours = sorted(set(range(24)) - set(self.actual_hours))\n",
    "        if missing_hours:\n",
    "            gaps = []\n",
    "            current_gap = [missing_hours[0]]\n",
    "            for i in range(1, len(missing_hours)):\n",
    "                if missing_hours[i] == missing_hours[i-1] + 1:\n",
    "                    current_gap.append(missing_hours[i])\n",
    "                else:\n",
    "                    gaps.append(current_gap)\n",
    "                    current_gap = [missing_hours[i]]\n",
    "            gaps.append(current_gap)\n",
    "            \n",
    "            print(f\"Recording gaps:\")\n",
    "            for gap in gaps:\n",
    "                if len(gap) == 1:\n",
    "                    print(f\"- Hour {gap[0]}\")\n",
    "                else:\n",
    "                    print(f\"- Hours {gap[0]}-{gap[-1]} ({len(gap)} hours)\")\n",
    "        \n",
    "        return self.actual_hours, hour_check\n",
    "    \n",
    "    def load_and_prepare_sparse_data(self, session):\n",
    "        \"\"\"Load data only for actual recording hours\"\"\"\n",
    "        \n",
    "        # Discover actual hours first\n",
    "        self.actual_hours, hour_stats = self.discover_actual_recording_hours(session)\n",
    "        \n",
    "        # Load data with hour filter\n",
    "        vitalise_sp = session.table(TABLE_NAME)\n",
    "        vitalise_sp = vitalise_sp.filter(col(PARENT_COL).is_not_null())\n",
    "        vitalise_sp = vitalise_sp.with_column(\"HOUR\", hour(col(TIME_COL)))\n",
    "        \n",
    "        # KEY FIX: Only include actual recording hours\n",
    "        vitalise_sp = vitalise_sp.filter(col(\"HOUR\").isin(self.actual_hours))\n",
    "        \n",
    "        agg_sp = vitalise_sp.group_by(\"HOUR\", PARENT_COL).count()\n",
    "        agg_df = agg_sp.to_pandas()\n",
    "        \n",
    "        print(f\"Data Overview:\")\n",
    "        print(f\"Records loaded: {len(agg_df):,}\")\n",
    "        print(f\"Hours included: {len(self.actual_hours)} ({self.actual_hours})\")\n",
    "        print(f\"Sound classes: {agg_df[PARENT_COL].nunique()}\")\n",
    "        \n",
    "        return agg_df\n",
    "    \n",
    "    def comprehensive_class_selection(self, agg_df, parent_col):\n",
    "        \"\"\"Smart class selection adapted for sparse hours\"\"\"\n",
    "        print(f\"SOUND CLASS SELECTION (Sparse Hours)\")\n",
    "        \n",
    "        print(f\"Analysing {agg_df[parent_col].nunique()} sound classes across {len(self.actual_hours)} hours...\")\n",
    "        \n",
    "        class_stats = agg_df.groupby(parent_col)['COUNT'].agg([\n",
    "            'sum', 'mean', 'std', 'count', 'min', 'max'\n",
    "        ]).round(2)\n",
    "        \n",
    "        total_activity = agg_df['COUNT'].sum()\n",
    "        class_stats['percentage'] = (class_stats['sum'] / total_activity * 100).round(3)\n",
    "        class_stats['cv'] = (class_stats['std'] / class_stats['mean']).round(3)\n",
    "        \n",
    "        print(f\"Applying Selection Criteria:\")\n",
    "        \n",
    "        # Criterion 1: Minimum occurrence (same)\n",
    "        occurrence_filter = class_stats['percentage'] >= (self.min_occurrence * 100)\n",
    "        print(f\"Occurrence ≥{self.min_occurrence*100}%: {occurrence_filter.sum()} classes\")\n",
    "        \n",
    "        # Criterion 2: Present in multiple hours (adapted for fewer hours)\n",
    "        temporal_spread = agg_df.groupby(parent_col)['HOUR'].nunique()\n",
    "        min_hours = max(2, len(self.actual_hours) // 3)  # At least 1/3 of available hours\n",
    "        spread_filter = temporal_spread >= min_hours\n",
    "        print(f\"Temporal spread ≥{min_hours} hours: {spread_filter.sum()} classes\")\n",
    "        \n",
    "        # Criterion 3: Some temporal variation (same)\n",
    "        variation_filter = class_stats['cv'] > 0.1\n",
    "        print(f\"Temporal variation (CV>0.1): {variation_filter.sum()} classes\")\n",
    "        \n",
    "        # Combine criteria\n",
    "        selected_classes = class_stats[\n",
    "            occurrence_filter & \n",
    "            spread_filter.reindex(class_stats.index, fill_value=False) &\n",
    "            variation_filter\n",
    "        ].sort_values('sum', ascending=False)\n",
    "        \n",
    "        print(f\"FINAL SELECTION: {len(selected_classes)} classes\")\n",
    "        print(f\"Coverage: {selected_classes['percentage'].sum():.1f}% of total activity\")\n",
    "        \n",
    "        return selected_classes, class_stats\n",
    "    \n",
    "    def create_feature_matrix(self, agg_df, selected_class_names):\n",
    "        print(f\"CREATING FEATURE MATRIX\")\n",
    "        \n",
    "        # Filter to selected classes\n",
    "        filtered_df = agg_df[agg_df[PARENT_COL].isin(selected_class_names)]\n",
    "        \n",
    "        # Create pivot - only actual hours, no padding!\n",
    "        df = filtered_df.pivot(index=\"HOUR\", columns=PARENT_COL, values=\"COUNT\").fillna(0)\n",
    "        \n",
    "        \n",
    "        print(f\"Feature Matrix:\")\n",
    "        print(f\"Shape: {df.shape} (Hours × Sound Classes)\")\n",
    "        print(f\"Hours included: {sorted(df.index.tolist())}\")\n",
    "        print(f\"Sparsity: {(df == 0).mean().mean():.1%} (real sparsity)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def apply_pca(self, df):\n",
    "        \"\"\"Apply PCA to real data matrix\"\"\"\n",
    "        \n",
    "        print(f\"Input matrix: {df.shape[0]} hours × {df.shape[1]} sound classes\")\n",
    "        \n",
    "        # Remove zero-variance features\n",
    "        self.variance_selector = VarianceThreshold(threshold=0)\n",
    "        df_cleaned = pd.DataFrame(\n",
    "            self.variance_selector.fit_transform(df),\n",
    "            columns=df.columns[self.variance_selector.get_support()],\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        print(f\"Features after variance filtering: {df_cleaned.shape[1]}\")\n",
    "        \n",
    "        # Standardize\n",
    "        df_scaled = self.scaler.fit_transform(df_cleaned)\n",
    "        \n",
    "        # Apply PCA\n",
    "        self.pca = PCA()\n",
    "        pca_components = self.pca.fit_transform(df_scaled)\n",
    "        \n",
    "        # Determine optimal components\n",
    "        self.optimal_components = self._determine_optimal_components()\n",
    "        \n",
    "        # Create results\n",
    "        pca_columns = [f'PC{i+1}' for i in range(self.optimal_components)]\n",
    "        pca_results = pd.DataFrame(\n",
    "            pca_components[:, :self.optimal_components],\n",
    "            columns=pca_columns,\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        loadings = pd.DataFrame(\n",
    "            self.pca.components_[:self.optimal_components].T,\n",
    "            columns=pca_columns,\n",
    "            index=df_cleaned.columns\n",
    "        )\n",
    "        \n",
    "        print(f\"PCA Results:\")\n",
    "        print(f\"Optimal components: {self.optimal_components}\")\n",
    "        print(f\"Variance explained: {self.pca.explained_variance_ratio_[:self.optimal_components].sum():.1%}\")\n",
    "        \n",
    "        return pca_results, loadings, df_cleaned\n",
    "    \n",
    "    def _determine_optimal_components(self):\n",
    "        \"\"\"Determine optimal components for sparse data\"\"\"\n",
    "        cumsum_var = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        var_threshold_comps = np.argmax(cumsum_var >= self.min_variance) + 1\n",
    "        kaiser_comps = np.sum(self.pca.explained_variance_ > 1)\n",
    "        \n",
    "        # For sparse data, be more conservative\n",
    "        optimal = min(max(var_threshold_comps, kaiser_comps), len(self.actual_hours) - 1)\n",
    "        \n",
    "        print(f\"Component Selection:\")\n",
    "        print(f\"Variance threshold ({self.min_variance:.0%}): {var_threshold_comps}\")\n",
    "        print(f\"Kaiser criterion (λ>1): {kaiser_comps}\")\n",
    "        print(f\"Selected (conservative): {optimal}\")\n",
    "        \n",
    "        return optimal\n",
    "    \n",
    "    def cluster_data(self, pca_results):\n",
    "        \"\"\"Cluster the PCA results\"\"\"\n",
    "        \n",
    "        # Find optimal clusters for sparse data\n",
    "        max_k = min(6, len(self.actual_hours) // 2)  # Conservative for sparse data\n",
    "        \n",
    "        silhouette_scores = []\n",
    "        k_range = range(2, max_k + 1)\n",
    "        \n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(pca_results)\n",
    "            score = silhouette_score(pca_results, labels)\n",
    "            silhouette_scores.append(score)\n",
    "        \n",
    "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "        best_score = max(silhouette_scores)\n",
    "        \n",
    "        # Final clustering\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(pca_results)\n",
    "        \n",
    "        print(f\"Clustering Results:\")\n",
    "        print(f\"Optimal clusters: {optimal_k}\")\n",
    "        print(f\"Silhouette score: {best_score:.3f}\")\n",
    "        print(f\"Hours per cluster: {pd.Series(cluster_labels).value_counts().sort_index().tolist()}\")\n",
    "        \n",
    "        # Interpret clusters\n",
    "        self._interpret_sparse_clusters(cluster_labels, pca_results)\n",
    "        \n",
    "        return cluster_labels, optimal_k, best_score\n",
    "    \n",
    "    def _interpret_sparse_clusters(self, cluster_labels, pca_results):\n",
    "        \"\"\"Interpret clusters for sparse temporal data\"\"\"\n",
    "        \n",
    "        for cluster_id in sorted(set(cluster_labels)):\n",
    "            mask = cluster_labels == cluster_id\n",
    "            cluster_hours = np.array(self.actual_hours)[mask]\n",
    "            \n",
    "            print(f\"Cluster {cluster_id} ({len(cluster_hours)} hours):\")\n",
    "            print(f\"Hours: {sorted(cluster_hours.tolist())}\")\n",
    "            \n",
    "            # Characterize time periods\n",
    "            morning_hours = [h for h in cluster_hours if 6 <= h <= 10]\n",
    "            afternoon_hours = [h for h in cluster_hours if 15 <= h <= 17]\n",
    "            evening_hours = [h for h in cluster_hours if 19 <= h <= 20]\n",
    "            \n",
    "            characteristics = []\n",
    "            if morning_hours:\n",
    "                characteristics.append(f\"Morning ({len(morning_hours)} hours)\")\n",
    "            if afternoon_hours:\n",
    "                characteristics.append(f\"Afternoon ({len(afternoon_hours)} hours)\")\n",
    "            if evening_hours:\n",
    "                characteristics.append(f\"Evening ({len(evening_hours)} hours)\")\n",
    "            \n",
    "            print(f\"Time periods: {', '.join(characteristics) if characteristics else 'Mixed'}\")\n",
    "    \n",
    "    def visualize_results(self, pca_results, cluster_labels, loadings):\n",
    "        \"\"\"Visualize analysis results\"\"\"\n",
    "\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. PCA scatter plot\n",
    "        scatter = axes[0,0].scatter(pca_results.iloc[:,0], pca_results.iloc[:,1], \n",
    "                                   c=cluster_labels, cmap='viridis', s=100, alpha=0.8)\n",
    "        axes[0,0].set_xlabel(f'PC1 ({self.pca.explained_variance_ratio_[0]:.1%})')\n",
    "        axes[0,0].set_ylabel(f'PC2 ({self.pca.explained_variance_ratio_[1]:.1%})')\n",
    "        axes[0,0].set_title('PCA Scatterplot of Daily Hours')\n",
    "        \n",
    "        # Add hour labels\n",
    "        for i, hour in enumerate(pca_results.index):\n",
    "            axes[0,0].annotate(f'{hour:02d}', (pca_results.iloc[i,0], pca_results.iloc[i,1]),\n",
    "                              xytext=(3, 3), textcoords='offset points', fontweight='bold')\n",
    "        \n",
    "        # 2. Explained variance\n",
    "        n_components = min(8, len(self.pca.explained_variance_ratio_))\n",
    "        axes[0,1].bar(range(1, n_components + 1), \n",
    "                     self.pca.explained_variance_ratio_[:n_components])\n",
    "        axes[0,1].set_xlabel('Component')\n",
    "        axes[0,1].set_ylabel('Explained Variance')\n",
    "        axes[0,1].set_title('PCA Explained Variance')\n",
    "        \n",
    "        # 3. Temporal pattern\n",
    "        colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "        for cluster_id in sorted(set(cluster_labels)):\n",
    "            mask = cluster_labels == cluster_id\n",
    "            cluster_hours = np.array(self.actual_hours)[mask]\n",
    "            cluster_activities = [1] * len(cluster_hours)  # Placeholder\n",
    "            \n",
    "            axes[1,0].scatter(cluster_hours, cluster_activities, \n",
    "                             color=colors[cluster_id % len(colors)], \n",
    "                             label=f'Cluster {cluster_id}', s=100, alpha=0.8)\n",
    "        \n",
    "        axes[1,0].set_xlabel('Hour of Day')\n",
    "        axes[1,0].set_ylabel('Cluster Assignment')\n",
    "        axes[1,0].set_title('Sparse Temporal Pattern')\n",
    "        axes[1,0].set_xticks(range(0, 24, 2))\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Top loadings\n",
    "        top_loadings = loadings['PC1'].abs().nlargest(10)\n",
    "        axes[1,1].barh(range(len(top_loadings)), top_loadings.values)\n",
    "        axes[1,1].set_yticks(range(len(top_loadings)))\n",
    "        axes[1,1].set_yticklabels(top_loadings.index, fontsize=8)\n",
    "        axes[1,1].set_title('Top PC1 Loadings')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def run_analysis(session):\n",
    "    \"\"\"Run the analysis without padding\"\"\"\n",
    "    \n",
    "    # Initialise analyzer\n",
    "    analyzer = PCAAnalyzer(min_variance=0.90, min_occurrence=0.001)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    agg_df = analyzer.load_and_prepare_sparse_data(session)\n",
    "    \n",
    "    # Step 2: Smart class selection\n",
    "    selected_classes, all_class_stats = analyzer.comprehensive_class_selection(agg_df, PARENT_COL)\n",
    "    \n",
    "    # Step 3: Create feature matrix\n",
    "    selected_class_names = selected_classes.index.tolist()\n",
    "    df = analyzer.create_feature_matrix(agg_df, selected_class_names)\n",
    "    \n",
    "    # Step 4: Apply PCA\n",
    "    pca_results, loadings, df_cleaned = analyzer.apply_pca(df)\n",
    "    \n",
    "    # Step 5: Cluster data\n",
    "    cluster_labels, optimal_k, silhouette_score = analyzer.cluster_data(pca_results)\n",
    "    \n",
    "    # Step 6: Visualize\n",
    "    analyzer.visualize_results(pca_results, cluster_labels, loadings)\n",
    "\n",
    "    \n",
    "    print(f\"ANALYSIS COMPLETE!\")\n",
    "    print(f\"Real hours analyzed: {len(analyzer.actual_hours)}\")\n",
    "    print(f\"Clusters found: {optimal_k}\")\n",
    "    print(f\"Clustering quality: {silhouette_score:.3f}\")\n",
    "    print(f\"Based on REAL patterns, not artificial padding!\")\n",
    "    \n",
    "    return analyzer, pca_results, cluster_labels, loadings\n",
    "\n",
    "# Run the analysis:\n",
    "analyzer, pca_results, clusters, loadings = run_analysis(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cca4d-bc45-40a8-8617-182950384a55",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "# Inspect PCA Components\n",
    "\n",
    "\n",
    "# 1. Variance explained by each component\n",
    "explained_variance = analyzer.pca.explained_variance_ratio_\n",
    "print(\"Explained variance by component:\")\n",
    "for i, var in enumerate(explained_variance[:10], 1):  # first 10 PCs\n",
    "    print(f\"PC{i}: {var:.2%}\")\n",
    "\n",
    "# 2. Top loadings for PC1 and PC2\n",
    "print(\"Top contributing sound classes for PC1:\")\n",
    "print(loadings['PC1'].sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"Top contributing sound classes for PC2:\")\n",
    "print(loadings['PC2'].sort_values(ascending=False).head(10))\n",
    "\n",
    "# 3. Bottom loadings (negative contributions)\n",
    "print(\"Negative contributors to PC1:\")\n",
    "print(loadings['PC1'].sort_values().head(10))\n",
    "\n",
    "print(\"Negative contributors to PC2:\")\n",
    "print(loadings['PC2'].sort_values().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40046004-2c4f-4bb4-9099-f2c4840a846d",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POST-PCA ANALYSIS SUITE\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from snowflake.snowpark.functions import col, hour\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 1. DEEP CLUSTER ANALYSIS\n",
    "\n",
    "\n",
    "class PostPCAAnalyzer:\n",
    "    \"\"\"Advanced analysis after PCA clustering\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer, pca_results, clusters, loadings, session):\n",
    "        self.analyzer = analyzer\n",
    "        self.pca_results = pca_results\n",
    "        self.clusters = clusters\n",
    "        self.loadings = loadings\n",
    "        self.session = session\n",
    "        self.cluster_profiles = {}\n",
    "        \n",
    "    def deep_cluster_analysis(self):\n",
    "        \"\"\"Comprehensive cluster characterization\"\"\"\n",
    "\n",
    "        # Create cluster profiles\n",
    "        for cluster_id in sorted(set(self.clusters)):\n",
    "            mask = self.clusters == cluster_id\n",
    "            cluster_hours = np.array(self.analyzer.actual_hours)[mask]\n",
    "            cluster_pca = self.pca_results[mask]\n",
    "            \n",
    "            profile = {\n",
    "                'hours': cluster_hours,\n",
    "                'size': len(cluster_hours),\n",
    "                'pca_mean': cluster_pca.mean(axis=0),\n",
    "                'pca_std': cluster_pca.std(axis=0),\n",
    "                'time_periods': self._categorize_time_periods(cluster_hours)\n",
    "            }\n",
    "            \n",
    "            self.cluster_profiles[cluster_id] = profile\n",
    "            \n",
    "            print(f\"CLUSTER {cluster_id} DETAILED PROFILE:\")\n",
    "            print(f\"Hours: {sorted(cluster_hours.tolist())}\")\n",
    "            print(f\"Size: {profile['size']} hours ({profile['size']/len(self.analyzer.actual_hours)*100:.1f}%)\")\n",
    "            print(f\"Time periods: {profile['time_periods']}\")\n",
    "            print(f\"PC1 mean: {profile['pca_mean'][0]:.3f} ± {profile['pca_std'][0]:.3f}\")\n",
    "            print(f\"PC2 mean: {profile['pca_mean'][1]:.3f} ± {profile['pca_std'][1]:.3f}\")\n",
    "        \n",
    "        return self.cluster_profiles\n",
    "    \n",
    "    def _categorize_time_periods(self, hours):\n",
    "        \"\"\"Categorize hours into time periods\"\"\"\n",
    "        periods = {\n",
    "            'Dawn': [h for h in hours if 6 <= h <= 7],\n",
    "            'Morning': [h for h in hours if 8 <= h <= 11],\n",
    "            'Midday': [h for h in hours if 12 <= h <= 14],\n",
    "            'Afternoon': [h for h in hours if 15 <= h <= 17],\n",
    "            'Evening': [h for h in hours if 18 <= h <= 20],\n",
    "            'Night': [h for h in hours if 21 <= h <= 23]\n",
    "        }\n",
    "        \n",
    "        return {period: len(period_hours) for period, period_hours in periods.items() if period_hours}\n",
    "    \n",
    "    def analyze_driving_sound_classes(self):\n",
    "        \"\"\"Identify which sound classes distinguish clusters\"\"\"\n",
    "        \n",
    "        # Get original data matrix\n",
    "        original_data = self._reconstruct_original_data()\n",
    "        \n",
    "        # Statistical tests between clusters\n",
    "        cluster_differences = {}\n",
    "        \n",
    "        for sound_class in original_data.columns:\n",
    "            cluster_0_data = original_data.loc[self.clusters == 0, sound_class]\n",
    "            cluster_1_data = original_data.loc[self.clusters == 1, sound_class]\n",
    "            \n",
    "            # Mann-Whitney U test (non-parametric)\n",
    "            statistic, p_value = stats.mannwhitneyu(cluster_0_data, cluster_1_data, \n",
    "                                                   alternative='two-sided')\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(cluster_0_data)-1)*cluster_0_data.var() + \n",
    "                                (len(cluster_1_data)-1)*cluster_1_data.var()) / \n",
    "                               (len(cluster_0_data) + len(cluster_1_data) - 2))\n",
    "            \n",
    "            cohens_d = abs(cluster_0_data.mean() - cluster_1_data.mean()) / pooled_std\n",
    "            \n",
    "            cluster_differences[sound_class] = {\n",
    "                'p_value': p_value,\n",
    "                'effect_size': cohens_d,\n",
    "                'cluster_0_mean': cluster_0_data.mean(),\n",
    "                'cluster_1_mean': cluster_1_data.mean(),\n",
    "                'difference': cluster_1_data.mean() - cluster_0_data.mean()\n",
    "            }\n",
    "        \n",
    "        # Sort by effect size\n",
    "        sorted_diffs = sorted(cluster_differences.items(), \n",
    "                            key=lambda x: x[1]['effect_size'], reverse=True)\n",
    "        \n",
    "        print(f\"TOP 10 DISCRIMINATING SOUND CLASSES:\")\n",
    "        print(f\"{'Sound Class':<25} {'Effect Size':<12} {'P-value':<10} {'Cluster Preference'}\")\n",
    "\n",
    "        \n",
    "        for sound_class, stats_dict in sorted_diffs[:10]:\n",
    "            p_val = stats_dict['p_value']\n",
    "            effect = stats_dict['effect_size']\n",
    "            \n",
    "            # Determine which cluster has higher activity\n",
    "            if stats_dict['difference'] > 0:\n",
    "                preference = f\"Cluster 1 ({stats_dict['cluster_1_mean']:.1f} vs {stats_dict['cluster_0_mean']:.1f})\"\n",
    "            else:\n",
    "                preference = f\"Cluster 0 ({stats_dict['cluster_0_mean']:.1f} vs {stats_dict['cluster_1_mean']:.1f})\"\n",
    "            \n",
    "            significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n",
    "            \n",
    "            print(f\"{sound_class:<25} {effect:.3f}{significance:<9} {p_val:<10.4f} {preference}\")\n",
    "        \n",
    "        return cluster_differences\n",
    "    \n",
    "    def _reconstruct_original_data(self):\n",
    "        \"\"\"Reconstruct original data matrix from analyzer\"\"\"\n",
    "        \n",
    "        try:\n",
    "            vitalise_sp = self.session.table(\"vitalise_data_light_01\")\n",
    "            vitalise_sp = vitalise_sp.filter(col(\"PARENT_NAME\").is_not_null())\n",
    "            vitalise_sp = vitalise_sp.with_column(\"HOUR\", hour(col(\"TIME_COL\")))\n",
    "            vitalise_sp = vitalise_sp.filter(col(\"HOUR\").isin(self.analyzer.actual_hours))\n",
    "            \n",
    "            agg_sp = vitalise_sp.group_by(\"HOUR\", \"PARENT_NAME\").count()\n",
    "            agg_df = agg_sp.to_pandas()\n",
    "            \n",
    "            # Get the same selected classes as before\n",
    "            selected_classes = self.loadings.index.tolist()\n",
    "            filtered_df = agg_df[agg_df[\"PARENT_NAME\"].isin(selected_classes)]\n",
    "            \n",
    "            df = filtered_df.pivot(index=\"HOUR\", columns=\"PARENT_NAME\", values=\"COUNT\").fillna(0)\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not reconstruct original data. Using PCA approximation. Error: {e}\")\n",
    "            approx_data = np.dot(self.pca_results.values, self.loadings.T.values)\n",
    "            \n",
    "            return pd.DataFrame(\n",
    "                approx_data, \n",
    "                index=self.pca_results.index,\n",
    "                columns=self.loadings.index\n",
    "            )\n",
    "\n",
    "\n",
    "def analyze_temporal_transitions(analyzer, clusters):\n",
    "    \"\"\"Analyse transitions between acoustic states\"\"\"\n",
    "    \n",
    "    hours = np.array(analyzer.actual_hours)\n",
    "    \n",
    "    # Find transitions\n",
    "    transitions = []\n",
    "    for i in range(len(clusters) - 1):\n",
    "        if hours[i+1] == hours[i] + 1:  # Consecutive hours\n",
    "            if clusters[i] != clusters[i+1]:\n",
    "                transitions.append({\n",
    "                    'from_hour': hours[i],\n",
    "                    'to_hour': hours[i+1],\n",
    "                    'from_cluster': clusters[i],\n",
    "                    'to_cluster': clusters[i+1]\n",
    "                })\n",
    "    \n",
    "    print(f\"CLUSTER TRANSITIONS FOUND: {len(transitions)}\")\n",
    "    for trans in transitions:\n",
    "        print(f\"Hour {trans['from_hour']} → {trans['to_hour']}: \"\n",
    "              f\"Cluster {trans['from_cluster']} → {trans['to_cluster']}\")\n",
    "    \n",
    "    # Transition probabilities\n",
    "    if len(transitions) > 0:\n",
    "        transition_matrix = np.zeros((2, 2))\n",
    "        for trans in transitions:\n",
    "            transition_matrix[trans['from_cluster'], trans['to_cluster']] += 1\n",
    "        \n",
    "        # Normalize\n",
    "        row_sums = transition_matrix.sum(axis=1)\n",
    "        transition_matrix = transition_matrix / row_sums[:, np.newaxis]\n",
    "        \n",
    "        print(f\"TRANSITION PROBABILITY MATRIX:\")\n",
    "        print(f\"     Cluster 0  Cluster 1\")\n",
    "        print(f\"C0:    {transition_matrix[0,0]:.3f}     {transition_matrix[0,1]:.3f}\")\n",
    "        print(f\"C1:    {transition_matrix[1,0]:.3f}     {transition_matrix[1,1]:.3f}\")\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "\n",
    "# 3. ACOUSTIC DIVERSITY INDICES\n",
    "\n",
    "\n",
    "def calculate_acoustic_diversity_indices(session, analyzer):\n",
    "    \"\"\"Calculate standard acoustic ecology indices\"\"\"\n",
    "    \n",
    "    # Load hourly data\n",
    "    diversity_data = []\n",
    "    \n",
    "    try:\n",
    "        for hour in analyzer.actual_hours:\n",
    "            # Get data for this hour\n",
    "            hour_query = f\"\"\"\n",
    "            SELECT PARENT_NAME, COUNT(*) as activity\n",
    "            FROM vitalise_data_light_01 \n",
    "            WHERE PARENT_NAME IS NOT NULL \n",
    "            AND HOUR(TIME_COL) = {hour}\n",
    "            GROUP BY PARENT_NAME\n",
    "            \"\"\"\n",
    "            \n",
    "            hour_df = session.sql(hour_query).to_pandas()\n",
    "            \n",
    "            if len(hour_df) > 0:\n",
    "                # Shannon Diversity Index\n",
    "                total_activity = hour_df['ACTIVITY'].sum()\n",
    "                proportions = hour_df['ACTIVITY'] / total_activity\n",
    "                proportions = proportions[proportions > 0]  # Remove zeros for log calculation\n",
    "                shannon_div = -np.sum(proportions * np.log(proportions))\n",
    "                \n",
    "                # Simpson's Diversity Index\n",
    "                simpson_div = 1 - np.sum(proportions ** 2)\n",
    "                \n",
    "                # Species richness (number of sound classes)\n",
    "                richness = len(hour_df)\n",
    "                \n",
    "                # Evenness\n",
    "                max_shannon = np.log(richness) if richness > 1 else 1\n",
    "                evenness = shannon_div / max_shannon if max_shannon > 0 else 0\n",
    "                \n",
    "                diversity_data.append({\n",
    "                    'hour': hour,\n",
    "                    'shannon_diversity': shannon_div,\n",
    "                    'simpson_diversity': simpson_div,\n",
    "                    'richness': richness,\n",
    "                    'evenness': evenness,\n",
    "                    'total_activity': total_activity\n",
    "                })\n",
    "        \n",
    "        if diversity_data:\n",
    "            diversity_df = pd.DataFrame(diversity_data)\n",
    "            \n",
    "            print(f\"ACOUSTIC DIVERSITY SUMMARY:\")\n",
    "            print(f\"Shannon Diversity: {diversity_df['shannon_diversity'].mean():.3f} ± {diversity_df['shannon_diversity'].std():.3f}\")\n",
    "            print(f\"Simpson Diversity: {diversity_df['simpson_diversity'].mean():.3f} ± {diversity_df['simpson_diversity'].std():.3f}\")\n",
    "            print(f\"Average Richness: {diversity_df['richness'].mean():.1f} ± {diversity_df['richness'].std():.1f}\")\n",
    "            print(f\"Average Evenness: {diversity_df['evenness'].mean():.3f} ± {diversity_df['evenness'].std():.3f}\")\n",
    "            \n",
    "            return diversity_df\n",
    "        else:\n",
    "            print(\"Warning: No diversity data calculated\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not calculate diversity indices. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# 4. PREDICTIVE MODELING\n",
    "\n",
    "def build_cluster_prediction_model(pca_results, clusters, analyzer):\n",
    "    \"\"\"Build model to predict cluster membership\"\"\"\n",
    "    \n",
    "    # Prepare features\n",
    "    X = pca_results.values\n",
    "    y = clusters\n",
    "    \n",
    "    # Add temporal features\n",
    "    hours = np.array(analyzer.actual_hours).reshape(-1, 1)\n",
    "    \n",
    "    # Create cyclical time features\n",
    "    hour_sin = np.sin(2 * np.pi * hours / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hours / 24)\n",
    "    \n",
    "    # Combine features\n",
    "    X_enhanced = np.hstack([X, hour_sin, hour_cos])\n",
    "    feature_names = (list(pca_results.columns) + ['hour_sin', 'hour_cos'])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_enhanced, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf_model, X_enhanced, y, cv=5)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    print(f\"MODEL PERFORMANCE:\")\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    print(f\"Test accuracy: {(y_pred == y_test).mean():.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"TOP FEATURE IMPORTANCES:\")\n",
    "    for _, row in feature_importance.head(8).iterrows():\n",
    "        print(f\"   • {row['feature']}: {row['importance']:.3f}\")\n",
    "    \n",
    "    return rf_model, feature_importance\n",
    "\n",
    "# 5. VISUALIZATION SUITE\n",
    "\n",
    "\n",
    "def create_comprehensive_visualizations(analyzer, pca_results, clusters, loadings, diversity_df=None):\n",
    "    \"\"\"Create visualization suite\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Enhanced PCA plot with annotations\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    \n",
    "    for cluster_id in sorted(set(clusters)):\n",
    "        mask = clusters == cluster_id\n",
    "        cluster_hours = np.array(analyzer.actual_hours)[mask]\n",
    "        cluster_pca = pca_results[mask]\n",
    "        \n",
    "        ax1.scatter(cluster_pca.iloc[:,0], cluster_pca.iloc[:,1], \n",
    "                   c=colors[cluster_id], label=f'Cluster {cluster_id}', \n",
    "                   s=100, alpha=0.7, edgecolors='black')\n",
    "        \n",
    "        # Add hour labels\n",
    "        for i, hour in enumerate(cluster_hours):\n",
    "            ax1.annotate(f'{hour:02d}', \n",
    "                        (cluster_pca.iloc[i,0], cluster_pca.iloc[i,1]),\n",
    "                        xytext=(3, 3), textcoords='offset points', \n",
    "                        fontweight='bold', fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel(f'PC1 ({analyzer.pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax1.set_ylabel(f'PC2 ({analyzer.pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax1.set_title('Enhanced PCA Clustering')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Temporal pattern heatmap\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    \n",
    "    # Create a 24-hour template\n",
    "    full_day = np.zeros(24)\n",
    "    for i, hour in enumerate(analyzer.actual_hours):\n",
    "        full_day[hour] = clusters[i]\n",
    "    \n",
    "    # Reshape for heatmap\n",
    "    heatmap_data = full_day.reshape(1, -1)\n",
    "    \n",
    "    im = ax2.imshow(heatmap_data, cmap='viridis', aspect='auto', \n",
    "                   extent=[0, 24, -0.5, 0.5])\n",
    "    ax2.set_xticks(range(0, 25, 2))\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_title('24-Hour Cluster Pattern')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2, orientation='horizontal', pad=0.1)\n",
    "    cbar.set_label('Cluster')\n",
    "    \n",
    "    # 3. PC loadings heatmap\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    \n",
    "    # Top loadings for first 3 PCs\n",
    "    top_loadings = loadings.iloc[:15, :3]  # Top 15 classes, first 3 PCs\n",
    "    \n",
    "    sns.heatmap(top_loadings.T, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "                center=0, ax=ax3, cbar_kws={'label': 'Loading'})\n",
    "    ax3.set_title('PC Loadings Heatmap')\n",
    "    ax3.set_xlabel('Sound Classes')\n",
    "    \n",
    "    # 4. Explained variance\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    \n",
    "    n_components = min(10, len(analyzer.pca.explained_variance_ratio_))\n",
    "    cumvar = np.cumsum(analyzer.pca.explained_variance_ratio_[:n_components])\n",
    "    \n",
    "    ax4.bar(range(1, n_components + 1), \n",
    "           analyzer.pca.explained_variance_ratio_[:n_components], \n",
    "           alpha=0.7, color='skyblue', label='Individual')\n",
    "    ax4.plot(range(1, n_components + 1), cumvar, 'ro-', \n",
    "            color='red', label='Cumulative')\n",
    "    ax4.axhline(y=0.9, color='green', linestyle='--', label='90% threshold')\n",
    "    ax4.set_xlabel('Component')\n",
    "    ax4.set_ylabel('Explained Variance')\n",
    "    ax4.set_title('PCA Explained Variance')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cluster comparison radar chart\n",
    "    ax5 = plt.subplot(3, 4, 5, projection='polar')\n",
    "    \n",
    "    # Top 8 sound classes by loading magnitude\n",
    "    top_classes = loadings['PC1'].abs().nlargest(8).index\n",
    "    angles = np.linspace(0, 2*np.pi, len(top_classes), endpoint=False)\n",
    "    \n",
    "    try:\n",
    "        # Try to get mean values for each cluster from original data\n",
    "        # Note: This is a simplified approach - you may need to adapt\n",
    "        original_data = analyzer._reconstruct_original_data() if hasattr(analyzer, '_reconstruct_original_data') else None\n",
    "        \n",
    "        if original_data is not None:\n",
    "            # Use original data if available\n",
    "            data_to_use = original_data[top_classes]\n",
    "        else:\n",
    "            # Fallback: use PCA loadings as proxy\n",
    "            print(\"Using PCA loadings as proxy for radar chart\")\n",
    "            data_to_use = pd.DataFrame(\n",
    "                np.abs(loadings.loc[top_classes, 'PC1'].values).reshape(1, -1),\n",
    "                columns=top_classes,\n",
    "                index=[0]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Using simplified radar chart. Error: {e}\")\n",
    "        # Simple fallback using loadings\n",
    "        data_to_use = pd.DataFrame(\n",
    "            np.abs(loadings.loc[top_classes, 'PC1'].values).reshape(1, -1),\n",
    "            columns=top_classes,\n",
    "            index=[0]\n",
    "        )\n",
    "    \n",
    "    for cluster_id in sorted(set(clusters)):\n",
    "        if len(data_to_use) > 1:\n",
    "            mask = clusters == cluster_id\n",
    "            cluster_means = data_to_use[mask].mean()\n",
    "        else:\n",
    "            # Fallback case\n",
    "            cluster_means = data_to_use.iloc[0]\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        if data_to_use.max().max() > data_to_use.min().min():\n",
    "            cluster_means_norm = (cluster_means - data_to_use.min().min()) / (data_to_use.max().max() - data_to_use.min().min())\n",
    "        else:\n",
    "            cluster_means_norm = cluster_means / cluster_means.max() if cluster_means.max() > 0 else cluster_means\n",
    "        \n",
    "        ax5.plot(angles, cluster_means_norm, 'o-', \n",
    "                label=f'Cluster {cluster_id}', linewidth=2)\n",
    "        ax5.fill(angles, cluster_means_norm, alpha=0.1)\n",
    "    \n",
    "    ax5.set_xticks(angles)\n",
    "    ax5.set_xticklabels([cls[:15] + '...' if len(cls) > 15 else cls \n",
    "                        for cls in top_classes], fontsize=8)\n",
    "    ax5.set_title('Cluster Profiles\\n(Top Sound Classes)')\n",
    "    ax5.legend()\n",
    "    \n",
    "    # 6. Diversity indices (if available)\n",
    "    if diversity_df is not None:\n",
    "        ax6 = plt.subplot(3, 4, 6)\n",
    "        \n",
    "        colors_div = [colors[c] for c in clusters]\n",
    "        ax6.scatter(diversity_df['shannon_diversity'], diversity_df['simpson_diversity'],\n",
    "                   c=colors_div, s=100, alpha=0.7, edgecolors='black')\n",
    "        \n",
    "        ax6.set_xlabel('Shannon Diversity')\n",
    "        ax6.set_ylabel('Simpson Diversity')\n",
    "        ax6.set_title('Acoustic Diversity by Cluster')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Hierarchical clustering dendrogram\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    \n",
    "    try:\n",
    "        # Perform hierarchical clustering on PCA results\n",
    "        linkage_matrix = linkage(pca_results, method='ward')\n",
    "        \n",
    "        dendrogram(linkage_matrix, ax=ax7, \n",
    "                  labels=[f'H{h:02d}' for h in analyzer.actual_hours],\n",
    "                  orientation='top')\n",
    "        ax7.set_title('Hierarchical Clustering')\n",
    "        ax7.set_xlabel('Hours')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create dendrogram. Error: {e}\")\n",
    "        ax7.text(0.5, 0.5, 'Hierarchical clustering\\nnot available\\n(insufficient data)', \n",
    "                ha='center', va='center', transform=ax7.transAxes)\n",
    "        ax7.set_title('Hierarchical Clustering')\n",
    "    \n",
    "    # 8. PC1 vs PC3 (alternative view)\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    \n",
    "    for cluster_id in sorted(set(clusters)):\n",
    "        mask = clusters == cluster_id\n",
    "        cluster_pca = pca_results[mask]\n",
    "        \n",
    "        ax8.scatter(cluster_pca.iloc[:,0], cluster_pca.iloc[:,2], \n",
    "                   c=colors[cluster_id], label=f'Cluster {cluster_id}', \n",
    "                   s=100, alpha=0.7)\n",
    "    \n",
    "    ax8.set_xlabel(f'PC1 ({analyzer.pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax8.set_ylabel(f'PC3 ({analyzer.pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax8.set_title('PC1 vs PC3')\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9-12. Individual cluster profiles\n",
    "    for i, cluster_id in enumerate(sorted(set(clusters))):\n",
    "        ax = plt.subplot(3, 4, 9 + i)\n",
    "        \n",
    "        mask = clusters == cluster_id\n",
    "        cluster_hours = np.array(analyzer.actual_hours)[mask]\n",
    "        \n",
    "        # Show cluster in context of full day\n",
    "        full_day_activity = np.zeros(24)\n",
    "        for hour in cluster_hours:\n",
    "            full_day_activity[hour] = 1\n",
    "        \n",
    "        colors_map = ['lightgray' if x == 0 else colors[cluster_id] for x in full_day_activity]\n",
    "        \n",
    "        ax.bar(range(24), np.ones(24), color=colors_map, alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlim(0, 23)\n",
    "        ax.set_xlabel('Hour of Day')\n",
    "        ax.set_title(f'Cluster {cluster_id} Temporal Profile')\n",
    "        ax.set_xticks(range(0, 24, 4))\n",
    "        \n",
    "        # Add text annotation\n",
    "        ax.text(0.02, 0.98, f'{len(cluster_hours)} hours', \n",
    "               transform=ax.transAxes, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# MAIN EXECUTION FUNCTION\n",
    "\n",
    "def run_post_pca_analysis(analyzer, pca_results, clusters, loadings, session):\n",
    "    \"\"\"Run post-PCA analysis\"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize post-PCA analyzer\n",
    "    post_analyzer = PostPCAAnalyzer(analyzer, pca_results, clusters, loadings, session)\n",
    "    \n",
    "    # 1. Deep cluster analysis\n",
    "    cluster_profiles = post_analyzer.deep_cluster_analysis()\n",
    "    \n",
    "    # 2. Sound class drivers\n",
    "    cluster_differences = post_analyzer.analyze_driving_sound_classes()\n",
    "    \n",
    "    # 3. Temporal transitions\n",
    "    transitions = analyze_temporal_transitions(analyzer, clusters)\n",
    "    \n",
    "    # 4. Acoustic diversity indices\n",
    "    diversity_df = calculate_acoustic_diversity_indices(session, analyzer)\n",
    "    \n",
    "    # 5. Predictive modeling\n",
    "    rf_model, feature_importance = build_cluster_prediction_model(pca_results, clusters, analyzer)\n",
    "    \n",
    "    # 6. Comprehensive visualizations\n",
    "    create_comprehensive_visualizations(analyzer, pca_results, clusters, loadings, diversity_df)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'post_analyzer': post_analyzer,\n",
    "        'cluster_profiles': cluster_profiles,\n",
    "        'cluster_differences': cluster_differences,\n",
    "        'transitions': transitions,\n",
    "        'diversity_df': diversity_df,\n",
    "        'rf_model': rf_model,\n",
    "        'feature_importance': feature_importance\n",
    "    }\n",
    "\n",
    "# EXECUTE POST-PCA ANALYSIS\n",
    "results = run_post_pca_analysis(analyzer, pca_results, clusters, loadings, session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "suneethapillai93@gmail.com",
   "authorId": "1222265290694",
   "authorName": "MEENUNAIR",
   "lastEditTime": 1757171857436,
   "notebookId": "3ttxagpybjpbtjf6tvig",
   "sessionId": "f699f059-3942-4a5b-8402-9631e022cd86"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
