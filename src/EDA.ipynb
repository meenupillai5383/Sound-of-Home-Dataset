{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# Get active session from snowflake.snowpark\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ceb812-48f6-4045-93e2-415e37854f47",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA) for \"The Sounds of Home\" Dataset\n",
    "\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import col, count, sum as sum_, avg, min as min_, max as max_\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for dissertation-quality figures\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    df = session.table(\"vitalise_data_light_01\")\n",
    "\n",
    "    # Basic dataset information\n",
    "    total_rows = df.count()\n",
    "    print(f\"Total number of sound event detections: {total_rows:,}\")\n",
    "    \n",
    "    # Get unique counts for categorical variables\n",
    "    unique_houses = df.select(\"HOUSE_NAME\").distinct().count()\n",
    "    unique_rooms = df.select(\"ROOM_NAME\").distinct().count()  \n",
    "    unique_classes = df.select(\"CLASS_NAME\").distinct().count()\n",
    "    unique_parents = df.select(\"PARENT_NAME\").distinct().count()\n",
    "    unique_files = df.select(\"FILENAME\").distinct().count()\n",
    "    \n",
    "    print(f\"Number of unique houses: {unique_houses}\")\n",
    "    print(f\"Number of unique rooms: {unique_rooms}\")\n",
    "    print(f\"Number of unique sound classes: {unique_classes}\")\n",
    "    print(f\"Number of unique parent categories: {unique_parents}\")\n",
    "    print(f\"Number of unique audio files: {unique_files}\")\n",
    "    \n",
    "    # Temporal coverage\n",
    "    date_range = df.select(\n",
    "        F.min(\"DATE_COL\").alias(\"START_DATE\"),\n",
    "        F.max(\"DATE_COL\").alias(\"END_DATE\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Recording period: {date_range['START_DATE']} to {date_range['END_DATE']}\")\n",
    "    \n",
    "    # Basic statistics for numerical columns\n",
    "    stats = df.select(\n",
    "        F.min(\"PROBABILITY\").alias(\"PROB_MIN\"),\n",
    "        F.max(\"PROBABILITY\").alias(\"PROB_MAX\"), \n",
    "        F.avg(\"PROBABILITY\").alias(\"PROB_AVG\"),\n",
    "        F.stddev(\"PROBABILITY\").alias(\"PROB_STDDEV\"),\n",
    "        F.min(\"FRAME_INDEX\").alias(\"FRAME_MIN\"),\n",
    "        F.max(\"FRAME_INDEX\").alias(\"FRAME_MAX\"),\n",
    "        F.avg(\"FRAME_INDEX\").alias(\"FRAME_AVG\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Probability Statistics:\")\n",
    "    print(f\"  Range: {stats['PROB_MIN']:.4f} - {stats['PROB_MAX']:.4f}\")\n",
    "    print(f\"  Mean: {stats['PROB_AVG']:.4f}\")\n",
    "    print(f\"  Standard Deviation: {stats['PROB_STDDEV']:.4f}\")\n",
    "    \n",
    "    print(f\"Frame Index Statistics:\")\n",
    "    print(f\"  Range: {stats['FRAME_MIN']} - {stats['FRAME_MAX']}\")\n",
    "    print(f\"  Average: {stats['FRAME_AVG']:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"2. DATA QUALITY ASSESSMENT\")\n",
    "    \n",
    "    # Check for missing values in all key columns\n",
    "    null_counts = df.select(\n",
    "        F.sum(F.when(F.col(\"CLASS_NAME\").isNull(), 1).otherwise(0)).alias(\"CLASS_NAME_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"PROBABILITY\").isNull(), 1).otherwise(0)).alias(\"PROBABILITY_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"HOUSE_NAME\").isNull(), 1).otherwise(0)).alias(\"HOUSE_NAME_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"ROOM_NAME\").isNull(), 1).otherwise(0)).alias(\"ROOM_NAME_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"DATE_COL\").isNull(), 1).otherwise(0)).alias(\"DATE_COL_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"PARENT_NAME\").isNull(), 1).otherwise(0)).alias(\"PARENT_NAME_NULLS\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"Missing Values Analysis:\")\n",
    "    for col_name, null_count in null_counts.as_dict().items():\n",
    "        percentage = (null_count / total_rows) * 100\n",
    "        print(f\"  {col_name}: {null_count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Check probability distribution bounds (should be 0-1)\n",
    "    prob_bounds_check = df.select(\n",
    "        F.sum(F.when(F.col(\"PROBABILITY\") < 0, 1).otherwise(0)).alias(\"NEGATIVE_PROBS\"),\n",
    "        F.sum(F.when(F.col(\"PROBABILITY\") > 1, 1).otherwise(0)).alias(\"OVER_ONE_PROBS\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Probability Bounds Check:\")\n",
    "    print(f\"  Values < 0: {prob_bounds_check['NEGATIVE_PROBS']}\")\n",
    "    print(f\"  Values > 1: {prob_bounds_check['OVER_ONE_PROBS']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"3. SPATIAL DISTRIBUTION ANALYSIS\")\n",
    "    \n",
    "    # House-level analysis\n",
    "    house_stats = df.group_by(\"HOUSE_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"DETECTION_COUNT\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_PROBABILITY\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"UNIQUE_CLASSES\"),\n",
    "        F.count_distinct(\"ROOM_NAME\").alias(\"ROOMS_COUNT\")\n",
    "    ).order_by(\"HOUSE_NAME\").collect()\n",
    "    \n",
    "    print(\"House-level Statistics:\")\n",
    "    print(\"House\\t\\tDetections\\tAvg_Prob\\tUnique_Classes\\tRooms\")\n",
    "    print(\"*\" * 65)\n",
    "    for row in house_stats:\n",
    "        print(f\"{row['HOUSE_NAME']}\\t\\t{row['DETECTION_COUNT']:,}\\t\\t{row['AVG_PROBABILITY']:.3f}\\t\\t{row['UNIQUE_CLASSES']}\\t\\t{row['ROOMS_COUNT']}\")\n",
    "    \n",
    "    # Room-level analysis\n",
    "    room_stats = df.group_by(\"ROOM_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"DETECTION_COUNT\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_PROBABILITY\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"UNIQUE_CLASSES\"),\n",
    "        F.count_distinct(\"HOUSE_NAME\").alias(\"HOUSE_COUNT\")\n",
    "    ).order_by(F.desc(\"DETECTION_COUNT\")).collect()\n",
    "    \n",
    "    print(f\"\\nRoom-level Statistics (Top 10):\")\n",
    "    print(\"Room\\t\\t\\tDetections\\tAvg_Prob\\tUnique_Classes\\tHouses\")\n",
    "    print(\"*\" * 70)\n",
    "    for row in room_stats[:10]:\n",
    "        print(f\"{row['ROOM_NAME']:<15}\\t{row['DETECTION_COUNT']:,}\\t\\t{row['AVG_PROBABILITY']:.3f}\\t\\t{row['UNIQUE_CLASSES']}\\t\\t{row['HOUSE_COUNT']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"*\"*60)\n",
    "    print(\"4. SOUND EVENT CLASS ANALYSIS\")\n",
    "    print(\"*\"*60)\n",
    "    \n",
    "    # Most frequent sound classes\n",
    "    class_freq = df.group_by(\"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\"),\n",
    "        F.count_distinct(\"HOUSE_NAME\").alias(\"HOUSE_COVERAGE\"),\n",
    "        F.count_distinct(\"ROOM_NAME\").alias(\"ROOM_COVERAGE\")\n",
    "    ).order_by(F.desc(\"FREQUENCY\")).collect()\n",
    "    \n",
    "    print(\"Top 15 Most Frequent Sound Classes:\")\n",
    "    print(\"Sound Class\\t\\t\\tFrequency\\tAvg_Confidence\\tHouse_Coverage\\tRoom_Coverage\")\n",
    "    print(\"*\" * 85)\n",
    "    for i, row in enumerate(class_freq[:15]):\n",
    "        freq_pct = (row['FREQUENCY'] / total_rows) * 100\n",
    "        print(f\"{row['CLASS_NAME']:<20}\\t{row['FREQUENCY']:,} ({freq_pct:.1f}%)\\t{row['AVG_CONFIDENCE']:.3f}\\t\\t{row['HOUSE_COVERAGE']}\\t\\t{row['ROOM_COVERAGE']}\")\n",
    "    \n",
    "    # Parent category analysis (filter out null values)\n",
    "    parent_stats = df.filter(F.col(\"PARENT_NAME\").isNotNull()).group_by(\"PARENT_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"CHILD_CLASSES\")\n",
    "    ).order_by(F.desc(\"FREQUENCY\")).collect()\n",
    "    \n",
    "    # Also check how many records have null parent names\n",
    "    null_parent_count = df.filter(F.col(\"PARENT_NAME\").isNull()).count()\n",
    "    null_parent_pct = (null_parent_count / total_rows) * 100\n",
    "    \n",
    "    print(f\"Parent Category Analysis (Top 10):\")\n",
    "    if null_parent_count > 0:\n",
    "        print(f\"Note: {null_parent_count:,} records ({null_parent_pct:.1f}%) have null parent categories\")\n",
    "    print(\"Parent Category\\t\\t\\tFrequency\\tAvg_Confidence\\tChild_Classes\")\n",
    "    print(\"*\" * 70)\n",
    "    for row in parent_stats[:10]:\n",
    "        freq_pct = (row['FREQUENCY'] / total_rows) * 100\n",
    "        parent_name = row['PARENT_NAME'] if row['PARENT_NAME'] is not None else \"NULL/UNKNOWN\"\n",
    "        print(f\"{parent_name:<25}\\t{row['FREQUENCY']:,} ({freq_pct:.1f}%)\\t{row['AVG_CONFIDENCE']:.3f}\\t\\t{row['CHILD_CLASSES']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"*\"*60)\n",
    "    print(\"5. TEMPORAL PATTERN ANALYSIS\")\n",
    "    print(\"*\"*60)\n",
    "    \n",
    "    # Daily pattern analysis\n",
    "    daily_pattern = df.group_by(\"DATE_COL\").agg(\n",
    "        F.count(\"*\").alias(\"TOTAL_DETECTIONS\")\n",
    "    ).order_by(\"DATE_COL\").collect()\n",
    "    \n",
    "    print(\"Daily Detection Patterns:\")\n",
    "    print(\"Date\\t\\t\\tDetections\")\n",
    "    print(\"*\" * 30)\n",
    "    for row in daily_pattern:\n",
    "        print(f\"{row['DATE_COL']}\\t\\t{row['TOTAL_DETECTIONS']:,}\")\n",
    "    \n",
    "    # Hourly pattern analysis (extract hour from TIME_COL)\n",
    "    hourly_pattern = df.select(\n",
    "        F.hour(F.to_time(\"TIME_COL\")).alias(\"HOUR\"),\n",
    "        \"*\"\n",
    "    ).group_by(\"HOUR\").agg(\n",
    "        F.count(\"*\").alias(\"HOURLY_DETECTIONS\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_HOURLY_CONFIDENCE\")\n",
    "    ).order_by(\"HOUR\").collect()\n",
    "    \n",
    "    print(f\"\\nHourly Activity Pattern:\")\n",
    "    print(\"Hour\\tDetections\\tAvg_Confidence\")\n",
    "    print(\"-\" * 35)\n",
    "    for row in hourly_pattern:\n",
    "        if row['HOUR'] is not None:\n",
    "            print(f\"{row['HOUR']:02d}:00\\t{row['HOURLY_DETECTIONS']:,}\\t\\t{row['AVG_HOURLY_CONFIDENCE']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"6. CONFIDENCE SCORE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Probability distribution analysis\n",
    "    prob_quantiles = df.select(\n",
    "        F.expr(\"PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P10\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P25\"), \n",
    "        F.expr(\"PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P50\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P75\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P90\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P95\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P99\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"Confidence Score Distribution (Quantiles):\")\n",
    "    for percentile, value in prob_quantiles.as_dict().items():\n",
    "        print(f\"  {percentile}: {value:.4f}\")\n",
    "    \n",
    "    # High confidence detections (>= 0.8)\n",
    "    high_conf_analysis = df.filter(F.col(\"PROBABILITY\") >= 0.8).group_by(\"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"HIGH_CONF_COUNT\")\n",
    "    ).order_by(F.desc(\"HIGH_CONF_COUNT\")).collect()\n",
    "    \n",
    "    high_conf_total = sum([row['HIGH_CONF_COUNT'] for row in high_conf_analysis])\n",
    "    high_conf_percentage = (high_conf_total / total_rows) * 100\n",
    "    \n",
    "    print(f\"\\nHigh Confidence Detections (>= 0.8): {high_conf_total:,} ({high_conf_percentage:.1f}%)\")\n",
    "    print(\"Top 10 Sound Classes with High Confidence:\")\n",
    "    print(\"Sound Class\\t\\t\\tHigh Conf Count\\t% of Total High Conf\")\n",
    "    print(\"-\" * 60)\n",
    "    for row in high_conf_analysis[:10]:\n",
    "        pct_of_high_conf = (row['HIGH_CONF_COUNT'] / high_conf_total) * 100\n",
    "        print(f\"{row['CLASS_NAME']:<25}\\t{row['HIGH_CONF_COUNT']:,}\\t\\t{pct_of_high_conf:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"*\"*60)\n",
    "    print(\"7. RESEARCH INSIGHTS AND IMPLICATIONS\")\n",
    "    print(\"*\"*60)\n",
    "    \n",
    "    # Calculate key metrics for research discussion\n",
    "    avg_detections_per_file = total_rows / unique_files\n",
    "    avg_classes_per_house = sum([row['UNIQUE_CLASSES'] for row in house_stats]) / len(house_stats)\n",
    "    \n",
    "    print(\"Key Research Metrics:\")\n",
    "    print(f\"  Average detections per audio file: {avg_detections_per_file:.1f}\")\n",
    "    print(f\"  Average unique sound classes per house: {avg_classes_per_house:.1f}\")\n",
    "    print(f\"  Dataset density: {total_rows / (unique_houses * unique_files):.1f} detections per house-file combination\")\n",
    "    \n",
    "    # Identify most distinctive sounds per room type\n",
    "    print(f\"\\nRoom-Specific Sound Characteristics:\")\n",
    "    room_specific_sounds = df.group_by(\"ROOM_NAME\", \"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"COUNT\")\n",
    "    ).group_by(\"ROOM_NAME\").agg(\n",
    "        F.max(\"COUNT\").alias(\"MAX_COUNT\"),\n",
    "        F.sum(\"COUNT\").alias(\"TOTAL_COUNT\")\n",
    "    ).collect()\n",
    "    \n",
    "    for room_stat in room_specific_sounds[:5]:\n",
    "        dominance_ratio = room_stat['MAX_COUNT'] / room_stat['TOTAL_COUNT']\n",
    "        print(f\"  {room_stat['ROOM_NAME']}: Sound dominance ratio = {dominance_ratio:.3f}\")\n",
    "    \n",
    "    print(f\"\\nDataset Quality Indicators:\")\n",
    "    print(f\"  Completeness: {100 - (sum(null_counts.as_dict().values()) / (total_rows * len(null_counts.as_dict()))) * 100:.1f}%\")\n",
    "    print(f\"  Confidence reliability: {high_conf_percentage:.1f}% high-confidence detections\")\n",
    "    print(f\"  Spatial coverage: {unique_houses} homes, {unique_rooms} room types\")\n",
    "    print(f\"  Temporal coverage: Multi-day recordings with {len(daily_pattern)} recording days\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"*\"*80)\n",
    "    print(\"EDA COMPLETE - Dataset is ready for advanced machine learning analysis\")\n",
    "    print(\"*\"*80)\n",
    "\n",
    "# Additional utility functions for extended analysis\n",
    "def export_summary_statistics():\n",
    "    \"\"\"Export key statistics for dissertation appendix\"\"\"\n",
    "    df = session.table(\"vitalise_data_light_01\")\n",
    "    \n",
    "    # Create comprehensive summary table\n",
    "    summary_stats = df.select(\n",
    "        F.count(\"*\").alias(\"total_records\"),\n",
    "        F.count_distinct(\"HOUSE_NAME\").alias(\"unique_houses\"),\n",
    "        F.count_distinct(\"ROOM_NAME\").alias(\"unique_rooms\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"unique_sound_classes\"),\n",
    "        F.min(\"PROBABILITY\").alias(\"min_confidence\"),\n",
    "        F.max(\"PROBABILITY\").alias(\"max_confidence\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"mean_confidence\"),\n",
    "        F.stddev(\"PROBABILITY\").alias(\"std_confidence\")\n",
    "    ).collect()\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "def analyze_sound_event_sequences():\n",
    "    \"\"\"Analyse sequential patterns in sound events - useful for ADL research\"\"\"\n",
    "    df = session.table(\"vitalise_data_light_01\")\n",
    "    \n",
    "    # Get sequences of sound events within each file\n",
    "    sequences = df.select(\n",
    "        \"FILENAME\", \"HOUSE_NAME\", \"ROOM_NAME\", \n",
    "        \"FRAME_INDEX\", \"CLASS_NAME\", \"PROBABILITY\"\n",
    "    ).order_by(\"FILENAME\", \"FRAME_INDEX\").collect()\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Run the main EDA function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4753c2-64a3-42ae-b9df-7472e5cded71",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# Comprehensive Exploratory Data Analysis (EDA) for \"The Sounds of Home\" Dataset\n",
    "\n",
    "import snowflake.snowpark as snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import col, count, sum as sum_, avg, min as min_, max as max_\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style for dissertation-quality figures\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main EDA function for The Sounds of Home dataset\n",
    "    \n",
    "    This analysis focuses on:\n",
    "    1. Dataset overview and completeness\n",
    "    2. Temporal patterns in domestic soundscapes\n",
    "    3. Sound event classification analysis\n",
    "    4. Spatial distribution across homes and rooms\n",
    "    5. Probability distribution analysis\n",
    "    6. Data quality assessment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = session.table(\"vitalise_data_light_01\")\n",
    "    \n",
    "    # Basic dataset information\n",
    "    total_rows = df.count()\n",
    "    print(f\"Total number of sound event detections: {total_rows:,}\")\n",
    "    \n",
    "    # Get unique counts for categorical variables\n",
    "    unique_houses = df.select(\"HOUSE_NAME\").distinct().count()\n",
    "    unique_rooms = df.select(\"ROOM_NAME\").distinct().count()  \n",
    "    unique_classes = df.select(\"CLASS_NAME\").distinct().count()\n",
    "    unique_parents = df.select(\"PARENT_NAME\").distinct().count()\n",
    "    unique_files = df.select(\"FILENAME\").distinct().count()\n",
    "    \n",
    "    print(f\"Number of unique houses: {unique_houses}\")\n",
    "    print(f\"Number of unique rooms: {unique_rooms}\")\n",
    "    print(f\"Number of unique sound classes: {unique_classes}\")\n",
    "    print(f\"Number of unique parent categories: {unique_parents}\")\n",
    "    print(f\"Number of unique audio files: {unique_files}\")\n",
    "    \n",
    "    # Temporal coverage\n",
    "    date_range = df.select(\n",
    "        F.min(\"DATE_COL\").alias(\"START_DATE\"),\n",
    "        F.max(\"DATE_COL\").alias(\"END_DATE\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Recording period: {date_range['START_DATE']} to {date_range['END_DATE']}\")\n",
    "    \n",
    "    # Basic statistics for numerical columns\n",
    "    stats = df.select(\n",
    "        F.min(\"PROBABILITY\").alias(\"PROB_MIN\"),\n",
    "        F.max(\"PROBABILITY\").alias(\"PROB_MAX\"), \n",
    "        F.avg(\"PROBABILITY\").alias(\"PROB_AVG\"),\n",
    "        F.stddev(\"PROBABILITY\").alias(\"PROB_STDDEV\"),\n",
    "        F.min(\"FRAME_INDEX\").alias(\"FRAME_MIN\"),\n",
    "        F.max(\"FRAME_INDEX\").alias(\"FRAME_MAX\"),\n",
    "        F.avg(\"FRAME_INDEX\").alias(\"FRAME_AVG\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Probability Statistics:\")\n",
    "    print(f\"  Range: {stats['PROB_MIN']:.4f} - {stats['PROB_MAX']:.4f}\")\n",
    "    print(f\"  Mean: {stats['PROB_AVG']:.4f}\")\n",
    "    print(f\"  Standard Deviation: {stats['PROB_STDDEV']:.4f}\")\n",
    "    \n",
    "    print(f\"Frame Index Statistics:\")\n",
    "    print(f\"  Range: {stats['FRAME_MIN']} - {stats['FRAME_MAX']}\")\n",
    "    print(f\"  Average: {stats['FRAME_AVG']:.2f}\")\n",
    "    \n",
    "    print(\"2. DATA QUALITY ASSESSMENT\")\n",
    "    \n",
    "    # Check for missing values in all key columns\n",
    "    null_counts = df.select(\n",
    "        F.sum(F.when(F.col(\"CLASS_NAME\").isNull(), 1).otherwise(0)).alias(\"CLASS_NAME_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"PROBABILITY\").isNull(), 1).otherwise(0)).alias(\"PROBABILITY_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"HOUSE_NAME\").isNull(), 1).otherwise(0)).alias(\"HOUSE_NAME_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"ROOM_NAME\").isNull(), 1).otherwise(0)).alias(\"ROOM_NAME_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"DATE_COL\").isNull(), 1).otherwise(0)).alias(\"DATE_COL_NULLS\"),\n",
    "        F.sum(F.when(F.col(\"PARENT_NAME\").isNull(), 1).otherwise(0)).alias(\"PARENT_NAME_NULLS\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"Missing Values Analysis:\")\n",
    "    for col_name, null_count in null_counts.as_dict().items():\n",
    "        percentage = (null_count / total_rows) * 100\n",
    "        print(f\"  {col_name}: {null_count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Check probability distribution bounds (should be 0-1)\n",
    "    prob_bounds_check = df.select(\n",
    "        F.sum(F.when(F.col(\"PROBABILITY\") < 0, 1).otherwise(0)).alias(\"NEGATIVE_PROBS\"),\n",
    "        F.sum(F.when(F.col(\"PROBABILITY\") > 1, 1).otherwise(0)).alias(\"OVER_ONE_PROBS\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Probability Bounds Check:\")\n",
    "    print(f\"  Values < 0: {prob_bounds_check['NEGATIVE_PROBS']}\")\n",
    "    print(f\"  Values > 1: {prob_bounds_check['OVER_ONE_PROBS']}\")\n",
    "    \n",
    "\n",
    "    print(\"3. SPATIAL DISTRIBUTION ANALYSIS\")\n",
    "\n",
    "    \n",
    "    # House-level analysis\n",
    "    house_stats = df.group_by(\"HOUSE_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"DETECTION_COUNT\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_PROBABILITY\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"UNIQUE_CLASSES\"),\n",
    "        F.count_distinct(\"ROOM_NAME\").alias(\"ROOMS_COUNT\")\n",
    "    ).order_by(\"HOUSE_NAME\").collect()\n",
    "    \n",
    "    print(\"House-level Statistics:\")\n",
    "    print(\"House\\t\\tDetections\\tAvg_Prob\\tUnique_Classes\\tRooms\")\n",
    "    print(\"*\" * 65)\n",
    "    for row in house_stats:\n",
    "        print(f\"{row['HOUSE_NAME']}\\t\\t{row['DETECTION_COUNT']:,}\\t\\t{row['AVG_PROBABILITY']:.3f}\\t\\t{row['UNIQUE_CLASSES']}\\t\\t{row['ROOMS_COUNT']}\")\n",
    "    \n",
    "    # Room-level analysis\n",
    "    room_stats = df.group_by(\"ROOM_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"DETECTION_COUNT\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_PROBABILITY\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"UNIQUE_CLASSES\"),\n",
    "        F.count_distinct(\"HOUSE_NAME\").alias(\"HOUSE_COUNT\")\n",
    "    ).order_by(F.desc(\"DETECTION_COUNT\")).collect()\n",
    "    \n",
    "    print(f\"Room-level Statistics (Top 10):\")\n",
    "    print(\"Room\\t\\t\\tDetections\\tAvg_Prob\\tUnique_Classes\\tHouses\")\n",
    "    print(\"*\" * 70)\n",
    "    for row in room_stats[:10]:\n",
    "        print(f\"{row['ROOM_NAME']:<15}\\t{row['DETECTION_COUNT']:,}\\t\\t{row['AVG_PROBABILITY']:.3f}\\t\\t{row['UNIQUE_CLASSES']}\\t\\t{row['HOUSE_COUNT']}\")\n",
    "    \n",
    "    print(\"4. SOUND EVENT CLASS ANALYSIS\")\n",
    "\n",
    "    \n",
    "    # Most frequent sound classes\n",
    "    class_freq = df.group_by(\"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\"),\n",
    "        F.count_distinct(\"HOUSE_NAME\").alias(\"HOUSE_COVERAGE\"),\n",
    "        F.count_distinct(\"ROOM_NAME\").alias(\"ROOM_COVERAGE\")\n",
    "    ).order_by(F.desc(\"FREQUENCY\")).collect()\n",
    "    \n",
    "    print(\"Top 15 Most Frequent Sound Classes:\")\n",
    "    print(\"Sound Class\\t\\t\\tFrequency\\tAvg_Confidence\\tHouse_Coverage\\tRoom_Coverage\")\n",
    "    print(\"*\" * 85)\n",
    "    for i, row in enumerate(class_freq[:15]):\n",
    "        freq_pct = (row['FREQUENCY'] / total_rows) * 100\n",
    "        print(f\"{row['CLASS_NAME']:<20}\\t{row['FREQUENCY']:,} ({freq_pct:.1f}%)\\t{row['AVG_CONFIDENCE']:.3f}\\t\\t{row['HOUSE_COVERAGE']}\\t\\t{row['ROOM_COVERAGE']}\")\n",
    "    \n",
    "    # Parent category analysis (filter out null values)\n",
    "    parent_stats = df.filter(F.col(\"PARENT_NAME\").isNotNull()).group_by(\"PARENT_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"CHILD_CLASSES\")\n",
    "    ).order_by(F.desc(\"FREQUENCY\")).collect()\n",
    "    \n",
    "    # checking how many records have null parent names\n",
    "    null_parent_count = df.filter(F.col(\"PARENT_NAME\").isNull()).count()\n",
    "    null_parent_pct = (null_parent_count / total_rows) * 100\n",
    "    \n",
    "    print(f\"\\nParent Category Analysis (Top 10):\")\n",
    "    if null_parent_count > 0:\n",
    "        print(f\"Note: {null_parent_count:,} records ({null_parent_pct:.1f}%) have null parent categories\")\n",
    "    print(\"Parent Category\\t\\t\\tFrequency\\tAvg_Confidence\\tChild_Classes\")\n",
    "    print(\"*\" * 70)\n",
    "    for row in parent_stats[:10]:\n",
    "        freq_pct = (row['FREQUENCY'] / total_rows) * 100\n",
    "        parent_name = row['PARENT_NAME'] if row['PARENT_NAME'] is not None else \"NULL/UNKNOWN\"\n",
    "        print(f\"{parent_name:<25}\\t{row['FREQUENCY']:,} ({freq_pct:.1f}%)\\t{row['AVG_CONFIDENCE']:.3f}\\t\\t{row['CHILD_CLASSES']}\")\n",
    "    \n",
    "    print(\"5. TEMPORAL PATTERN ANALYSIS\")\n",
    "\n",
    "    \n",
    "    # Daily pattern analysis\n",
    "    daily_pattern = df.group_by(\"DATE_COL\").agg(\n",
    "        F.count(\"*\").alias(\"TOTAL_DETECTIONS\")\n",
    "    ).order_by(\"DATE_COL\").collect()\n",
    "    \n",
    "    print(\"Daily Detection Patterns:\")\n",
    "    print(\"Date\\t\\t\\tDetections\")\n",
    "    print(\"*\" * 30)\n",
    "    for row in daily_pattern:\n",
    "        print(f\"{row['DATE_COL']}\\t\\t{row['TOTAL_DETECTIONS']:,}\")\n",
    "    \n",
    "    # Hourly pattern analysis (extract hour from TIME_COL)\n",
    "    hourly_pattern = df.select(\n",
    "        F.hour(F.to_time(\"TIME_COL\")).alias(\"HOUR\"),\n",
    "        \"*\"\n",
    "    ).group_by(\"HOUR\").agg(\n",
    "        F.count(\"*\").alias(\"HOURLY_DETECTIONS\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_HOURLY_CONFIDENCE\")\n",
    "    ).order_by(\"HOUR\").collect()\n",
    "    \n",
    "    print(f\"\\nHourly Activity Pattern:\")\n",
    "    print(\"Hour\\tDetections\\tAvg_Confidence\")\n",
    "    print(\"*\" * 35)\n",
    "    for row in hourly_pattern:\n",
    "        if row['HOUR'] is not None:\n",
    "            print(f\"{row['HOUR']:02d}:00\\t{row['HOURLY_DETECTIONS']:,}\\t\\t{row['AVG_HOURLY_CONFIDENCE']:.3f}\")\n",
    "    \n",
    "    print(\"6. CONFIDENCE SCORE ANALYSIS\")\n",
    "\n",
    "    # Probability distribution analysis\n",
    "    prob_quantiles = df.select(\n",
    "        F.expr(\"PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P10\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P25\"), \n",
    "        F.expr(\"PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P50\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P75\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P90\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P95\"),\n",
    "        F.expr(\"PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY PROBABILITY)\").alias(\"P99\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(\"Confidence Score Distribution (Quantiles):\")\n",
    "    for percentile, value in prob_quantiles.as_dict().items():\n",
    "        print(f\"  {percentile}: {value:.4f}\")\n",
    "    \n",
    "    # High confidence detections (>= 0.8)\n",
    "    high_conf_analysis = df.filter(F.col(\"PROBABILITY\") >= 0.8).group_by(\"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"HIGH_CONF_COUNT\")\n",
    "    ).order_by(F.desc(\"HIGH_CONF_COUNT\")).collect()\n",
    "    \n",
    "    high_conf_total = sum([row['HIGH_CONF_COUNT'] for row in high_conf_analysis])\n",
    "    high_conf_percentage = (high_conf_total / total_rows) * 100\n",
    "    \n",
    "    print(f\"\\nHigh Confidence Detections (>= 0.8): {high_conf_total:,} ({high_conf_percentage:.1f}%)\")\n",
    "    print(\"Top 10 Sound Classes with High Confidence:\")\n",
    "    print(\"Sound Class\\t\\t\\tHigh Conf Count\\t% of Total High Conf\")\n",
    "    print(\"*\" * 60)\n",
    "    for row in high_conf_analysis[:10]:\n",
    "        pct_of_high_conf = (row['HIGH_CONF_COUNT'] / high_conf_total) * 100\n",
    "        print(f\"{row['CLASS_NAME']:<25}\\t{row['HIGH_CONF_COUNT']:,}\\t\\t{pct_of_high_conf:.1f}%\")\n",
    "    \n",
    "\n",
    "    print(\"7. RESEARCH INSIGHTS AND IMPLICATIONS\")\n",
    "\n",
    "    # Calculate key metrics for research discussion\n",
    "    avg_detections_per_file = total_rows / unique_files\n",
    "    avg_classes_per_house = sum([row['UNIQUE_CLASSES'] for row in house_stats]) / len(house_stats)\n",
    "    \n",
    "    print(\"Key Research Metrics:\")\n",
    "    print(f\"  Average detections per audio file: {avg_detections_per_file:.1f}\")\n",
    "    print(f\"  Average unique sound classes per house: {avg_classes_per_house:.1f}\")\n",
    "    print(f\"  Dataset density: {total_rows / (unique_houses * unique_files):.1f} detections per house-file combination\")\n",
    "    \n",
    "    # Identify most distinctive sounds per room type\n",
    "    print(f\"\\nRoom-Specific Sound Characteristics:\")\n",
    "    room_specific_sounds = df.group_by(\"ROOM_NAME\", \"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"COUNT\")\n",
    "    ).group_by(\"ROOM_NAME\").agg(\n",
    "        F.max(\"COUNT\").alias(\"MAX_COUNT\"),\n",
    "        F.sum(\"COUNT\").alias(\"TOTAL_COUNT\")\n",
    "    ).collect()\n",
    "    \n",
    "    for room_stat in room_specific_sounds[:5]:\n",
    "        dominance_ratio = room_stat['MAX_COUNT'] / room_stat['TOTAL_COUNT']\n",
    "        print(f\"  {room_stat['ROOM_NAME']}: Sound dominance ratio = {dominance_ratio:.3f}\")\n",
    "    \n",
    "    print(f\"Dataset Quality Indicators:\")\n",
    "    print(f\"  Completeness: {100 - (sum(null_counts.as_dict().values()) / (total_rows * len(null_counts.as_dict()))) * 100:.1f}%\")\n",
    "    print(f\"  Confidence reliability: {high_conf_percentage:.1f}% high-confidence detections\")\n",
    "    print(f\"  Spatial coverage: {unique_houses} homes, {unique_rooms} room types\")\n",
    "    print(f\"  Temporal coverage: Multi-day recordings with {len(daily_pattern)} recording days\")\n",
    "\n",
    "    print(\"EDA COMPLETE - Dataset is ready for advanced machine learning analysis\")\n",
    "    \n",
    "    # Generate comprehensive visualizations\n",
    "    create_visualizations(df, total_rows)\n",
    "\n",
    "def create_visualizations(df, total_rows):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualisations\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.ioff()  # Turn off interactive mode for batch processing\n",
    "    \n",
    "    # 1. TEMPORAL PATTERNS VISUALISATION\n",
    "    print(\"1. Creating temporal pattern visualisations...\")\n",
    "    \n",
    "    # Daily activity heatmap by hour\n",
    "    hourly_data = df.select(\n",
    "        F.hour(F.to_time(\"TIME_COL\")).alias(\"HOUR\"),\n",
    "        \"DATE_COL\"\n",
    "    ).filter(F.col(\"HOUR\").isNotNull()).group_by(\"DATE_COL\", \"HOUR\").agg(\n",
    "        F.count(\"*\").alias(\"DETECTIONS\")\n",
    "    ).collect()\n",
    "    \n",
    "    if hourly_data:\n",
    "        # Convert to pandas for visualization\n",
    "        hourly_df = pd.DataFrame([row.asDict() for row in hourly_data])\n",
    "        hourly_pivot = hourly_df.pivot(index='DATE_COL', columns='HOUR', values='DETECTIONS')\n",
    "        hourly_pivot = hourly_pivot.fillna(0)\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "        sns.heatmap(hourly_pivot, annot=False, cmap='YlOrRd', cbar_kws={'label': 'Number of Detections'})\n",
    "        plt.title('Daily Activity Patterns: Sound Events by Hour and Date\\nThe Sounds of Home Dataset', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Hour of Day', fontsize=12)\n",
    "        plt.ylabel('Recording Date', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('temporal_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. SOUND CLASS DISTRIBUTION\n",
    "    print(\"2. Creating sound class distribution visualizations...\")\n",
    "    \n",
    "    # Top sound classes\n",
    "    top_classes = df.group_by(\"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\")\n",
    "    ).order_by(F.desc(\"FREQUENCY\")).limit(20).collect()\n",
    "    \n",
    "    if top_classes:\n",
    "        classes_df = pd.DataFrame([row.asDict() for row in top_classes])\n",
    "        classes_df['PERCENTAGE'] = (classes_df['FREQUENCY'] / total_rows) * 100\n",
    "        \n",
    "        # Create bar plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Frequency plot\n",
    "        sns.barplot(data=classes_df.head(15), y='CLASS_NAME', x='FREQUENCY', \n",
    "                   palette='viridis', ax=ax1)\n",
    "        ax1.set_title('Top 15 Most Frequent Sound Classes', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Number of Detections (log scale)', fontsize=12)\n",
    "        ax1.set_ylabel('Sound Class', fontsize=12)\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # Confidence vs Frequency scatter\n",
    "        sns.scatterplot(data=classes_df, x='FREQUENCY', y='AVG_CONFIDENCE', \n",
    "                       size='PERCENTAGE', alpha=0.7, ax=ax2)\n",
    "        ax2.set_title('Sound Class Confidence vs Frequency', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Detection Frequency (log scale)', fontsize=12)\n",
    "        ax2.set_ylabel('Average Confidence Score', fontsize=12)\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.legend(title='% of Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sound_class_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. CONFIDENCE SCORE DISTRIBUTIONS\n",
    "    print(\"3. Creating confidence score distribution visualizations...\")\n",
    "    \n",
    "    # Sample confidence scores for visualization (sample for performance)\n",
    "    confidence_sample = df.select(\"PROBABILITY\", \"CLASS_NAME\").sample(0.01).collect()\n",
    "    \n",
    "    if confidence_sample:\n",
    "        conf_df = pd.DataFrame([row.asDict() for row in confidence_sample])\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Overall distribution\n",
    "        ax1.hist(conf_df['PROBABILITY'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax1.axvline(conf_df['PROBABILITY'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {conf_df[\"PROBABILITY\"].mean():.3f}')\n",
    "        ax1.set_title('Overall Confidence Score Distribution', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Confidence Score', fontsize=10)\n",
    "        ax1.set_ylabel('Frequency', fontsize=10)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Box plot by confidence ranges\n",
    "        conf_df['CONFIDENCE_RANGE'] = pd.cut(conf_df['PROBABILITY'], \n",
    "                                           bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                           labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "        conf_range_counts = conf_df['CONFIDENCE_RANGE'].value_counts()\n",
    "        ax2.pie(conf_range_counts.values, labels=conf_range_counts.index, autopct='%1.1f%%',\n",
    "               colors=sns.color_palette(\"husl\", len(conf_range_counts)))\n",
    "        ax2.set_title('Distribution by Confidence Ranges', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Confidence by top sound classes\n",
    "        top_classes_conf = conf_df[conf_df['CLASS_NAME'].isin(\n",
    "            conf_df['CLASS_NAME'].value_counts().head(10).index)]\n",
    "        sns.boxplot(data=top_classes_conf, x='PROBABILITY', y='CLASS_NAME', ax=ax3)\n",
    "        ax3.set_title('Confidence Distribution by Top Sound Classes', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Confidence Score', fontsize=10)\n",
    "        ax3.set_ylabel('Sound Class', fontsize=10)\n",
    "        \n",
    "        # Cumulative distribution\n",
    "        sorted_conf = np.sort(conf_df['PROBABILITY'])\n",
    "        cumulative = np.arange(1, len(sorted_conf) + 1) / len(sorted_conf)\n",
    "        ax4.plot(sorted_conf, cumulative, linewidth=2)\n",
    "        ax4.set_title('Cumulative Confidence Score Distribution', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Confidence Score', fontsize=10)\n",
    "        ax4.set_ylabel('Cumulative Probability', fontsize=10)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Confidence Score Analysis - The Sounds of Home Dataset', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confidence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. SPATIAL DISTRIBUTION\n",
    "    print(\"4. Creating spatial distribution visualizations...\")\n",
    "    \n",
    "    # House comparison\n",
    "    house_data = df.group_by(\"HOUSE_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"TOTAL_DETECTIONS\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"UNIQUE_CLASSES\")\n",
    "    ).collect()\n",
    "    \n",
    "    if house_data:\n",
    "        house_df = pd.DataFrame([row.asDict() for row in house_data])\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Total detections by house\n",
    "        sns.barplot(data=house_df, x='HOUSE_NAME', y='TOTAL_DETECTIONS', \n",
    "                   palette='Set2', ax=ax1)\n",
    "        ax1.set_title('Total Sound Detections by House', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('House ID', fontsize=10)\n",
    "        ax1.set_ylabel('Number of Detections', fontsize=10)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Average confidence by house\n",
    "        sns.barplot(data=house_df, x='HOUSE_NAME', y='AVG_CONFIDENCE', \n",
    "                   palette='Set3', ax=ax2)\n",
    "        ax2.set_title('Average Confidence Score by House', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('House ID', fontsize=10)\n",
    "        ax2.set_ylabel('Average Confidence', fontsize=10)\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Unique classes by house\n",
    "        sns.barplot(data=house_df, x='HOUSE_NAME', y='UNIQUE_CLASSES', \n",
    "                   palette='Set1', ax=ax3)\n",
    "        ax3.set_title('Unique Sound Classes by House', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('House ID', fontsize=10)\n",
    "        ax3.set_ylabel('Number of Unique Classes', fontsize=10)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Scatter plot: Detections vs Confidence\n",
    "        sns.scatterplot(data=house_df, x='TOTAL_DETECTIONS', y='AVG_CONFIDENCE', \n",
    "                       size='UNIQUE_CLASSES', sizes=(100, 500), ax=ax4)\n",
    "        ax4.set_title('House Characteristics: Detections vs Confidence', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Total Detections', fontsize=10)\n",
    "        ax4.set_ylabel('Average Confidence', fontsize=10)\n",
    "        \n",
    "        plt.suptitle('Spatial Distribution Analysis - House Comparison', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('spatial_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 5. ROOM COMPARISON\n",
    "    print(\"5. Creating room comparison visualizations...\")\n",
    "    \n",
    "    # Room-specific sound patterns\n",
    "    room_class_data = df.group_by(\"ROOM_NAME\", \"CLASS_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\")\n",
    "    ).collect()\n",
    "    \n",
    "    if room_class_data:\n",
    "        room_class_df = pd.DataFrame([row.asDict() for row in room_class_data])\n",
    "        \n",
    "        # Get top classes for each room\n",
    "        top_kitchen = room_class_df[room_class_df['ROOM_NAME'] == 'Kitchen'].nlargest(10, 'FREQUENCY')\n",
    "        top_living = room_class_df[room_class_df['ROOM_NAME'] == 'Living Room'].nlargest(10, 'FREQUENCY')\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Kitchen top sounds\n",
    "        if not top_kitchen.empty:\n",
    "            sns.barplot(data=top_kitchen, y='CLASS_NAME', x='FREQUENCY', \n",
    "                       palette='Oranges_r', ax=ax1)\n",
    "            ax1.set_title('Top 10 Sound Classes in Kitchen', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Number of Detections', fontsize=12)\n",
    "            ax1.set_ylabel('Sound Class', fontsize=12)\n",
    "            ax1.set_xscale('log')\n",
    "        \n",
    "        # Living room top sounds\n",
    "        if not top_living.empty:\n",
    "            sns.barplot(data=top_living, y='CLASS_NAME', x='FREQUENCY', \n",
    "                       palette='Blues_r', ax=ax2)\n",
    "            ax2.set_title('Top 10 Sound Classes in Living Room', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('Number of Detections', fontsize=12)\n",
    "            ax2.set_ylabel('Sound Class', fontsize=12)\n",
    "            ax2.set_xscale('log')\n",
    "        \n",
    "        plt.suptitle('Room-Specific Sound Event Patterns', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('room_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 6. PARENT CATEGORY ANALYSIS\n",
    "    print(\"6. Creating parent category visualizations...\")\n",
    "    \n",
    "    parent_data = df.filter(F.col(\"PARENT_NAME\").isNotNull()).group_by(\"PARENT_NAME\").agg(\n",
    "        F.count(\"*\").alias(\"FREQUENCY\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"AVG_CONFIDENCE\")\n",
    "    ).order_by(F.desc(\"FREQUENCY\")).limit(15).collect()\n",
    "    \n",
    "    if parent_data:\n",
    "        parent_df = pd.DataFrame([row.asDict() for row in parent_data])\n",
    "        parent_df['PERCENTAGE'] = (parent_df['FREQUENCY'] / total_rows) * 100\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "        # Frequency plot\n",
    "        sns.barplot(data=parent_df, y='PARENT_NAME', x='FREQUENCY', \n",
    "                   palette='viridis', ax=ax1)\n",
    "        ax1.set_title('Top 15 Parent Categories by Frequency', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Number of Detections (log scale)', fontsize=12)\n",
    "        ax1.set_ylabel('Parent Category', fontsize=12)\n",
    "        ax1.set_xscale('log')\n",
    "        \n",
    "        # Donut chart for top categories\n",
    "        colors = sns.color_palette(\"husl\", len(parent_df))\n",
    "        wedges, texts, autotexts = ax2.pie(parent_df['FREQUENCY'], labels=parent_df['PARENT_NAME'], \n",
    "                                          autopct='%1.1f%%', colors=colors, pctdistance=0.85)\n",
    "        centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
    "        ax2.add_artist(centre_circle)\n",
    "        ax2.set_title('Distribution of Parent Categories', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Adjust text size\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_fontsize(8)\n",
    "        for text in texts:\n",
    "            text.set_fontsize(8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('parent_categories.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Generated plots:\")\n",
    "    print(\"1. temporal_heatmap.png - Daily activity patterns\")\n",
    "    print(\"2. sound_class_analysis.png - Sound class distribution and confidence\")\n",
    "    print(\"3. confidence_analysis.png - Comprehensive confidence score analysis\")\n",
    "    print(\"4. spatial_analysis.png - House-level comparisons\")\n",
    "    print(\"5. room_comparison.png - Room-specific sound patterns\")\n",
    "    print(\"6. parent_categories.png - Parent category analysis\")\n",
    "\n",
    "\n",
    "# Additional utility functions for extended analysis\n",
    "def export_summary_statistics():\n",
    "    \"\"\"Export key statistics\"\"\"\n",
    "    df = session.table(\"vitalise_data_light_01\")\n",
    "    \n",
    "    # Create comprehensive summary table\n",
    "    summary_stats = df.select(\n",
    "        F.count(\"*\").alias(\"total_records\"),\n",
    "        F.count_distinct(\"HOUSE_NAME\").alias(\"unique_houses\"),\n",
    "        F.count_distinct(\"ROOM_NAME\").alias(\"unique_rooms\"),\n",
    "        F.count_distinct(\"CLASS_NAME\").alias(\"unique_sound_classes\"),\n",
    "        F.min(\"PROBABILITY\").alias(\"min_confidence\"),\n",
    "        F.max(\"PROBABILITY\").alias(\"max_confidence\"),\n",
    "        F.avg(\"PROBABILITY\").alias(\"mean_confidence\"),\n",
    "        F.stddev(\"PROBABILITY\").alias(\"std_confidence\")\n",
    "    ).collect()\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "def analyze_sound_event_sequences():\n",
    "    \"\"\"Analyze sequential patterns in sound events - useful for ADL research\"\"\"\n",
    "    df = session.table(\"vitalise_data_light_01\")\n",
    "    \n",
    "    # Get sequences of sound events within each file\n",
    "    sequences = df.select(\n",
    "        \"FILENAME\", \"HOUSE_NAME\", \"ROOM_NAME\", \n",
    "        \"FRAME_INDEX\", \"CLASS_NAME\", \"PROBABILITY\"\n",
    "    ).order_by(\"FILENAME\", \"FRAME_INDEX\").collect()\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Run the main EDA function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59346bb-4e9e-430c-8aa1-fbf2359775f9",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "# Load the table\n",
    "df = session.table(\"vitalise_data_light_01\")\n",
    "\n",
    "# Extract hour from TIME_COL and count events\n",
    "hourly_counts = (\n",
    "    df.select(F.hour(F.to_time(\"TIME_COL\")).alias(\"HOUR\"))\n",
    "      .group_by(\"HOUR\")\n",
    "      .agg(F.count(\"*\").alias(\"DETECTIONS\"))\n",
    "      .order_by(\"HOUR\")\n",
    "      .to_pandas()\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"HOUR\", y=\"DETECTIONS\", data=hourly_counts, color=\"steelblue\")\n",
    "plt.title(\"Distribution of Sound Event Detections by Hour of Day\", fontsize=14)\n",
    "plt.xlabel(\"Hour of Day (023)\")\n",
    "plt.ylabel(\"Number of Detections\")\n",
    "plt.xticks(range(0,24))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08f9f6-f099-4fc7-9f8c-35c1d40a1995",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "# Load table\n",
    "df = session.table(\"vitalise_data_light_01\")\n",
    "\n",
    "# Hourly distribution per house\n",
    "house_hourly = (\n",
    "    df.select(\n",
    "        F.col(\"HOUSE_NAME\"),\n",
    "        F.hour(F.to_time(\"TIME_COL\")).alias(\"HOUR\")\n",
    "    )\n",
    "    .group_by(\"HOUSE_NAME\", \"HOUR\")\n",
    "    .agg(F.count(\"*\").alias(\"DETECTIONS\"))\n",
    "    .order_by(\"HOUSE_NAME\", \"HOUR\")\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# Plot: heatmap of hours vs houses\n",
    "pivot_table = house_hourly.pivot(index=\"HOUSE_NAME\", columns=\"HOUR\", values=\"DETECTIONS\").fillna(0)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.heatmap(\n",
    "    pivot_table, \n",
    "    cmap=\"YlGnBu\", \n",
    "    cbar_kws={'label': 'Number of Detections'}, \n",
    "    linewidths=0.3\n",
    ")\n",
    "plt.title(\"Hourly Distribution of Recordings by House\", fontsize=14)\n",
    "plt.xlabel(\"Hour of Day (023)\")\n",
    "plt.ylabel(\"House\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "suneethapillai93@gmail.com",
   "authorId": "1222265290694",
   "authorName": "MEENUNAIR",
   "lastEditTime": 1757170519871,
   "notebookId": "4f6shikin7lo3yca47yr",
   "sessionId": "4b06a8cc-e6bf-4558-bae9-e471d15acc5a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
